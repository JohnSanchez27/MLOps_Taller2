# PredicciÃ³n de Especies de PingÃ¼inos con FastAPI y Streamlit

## ğŸ“Œ DescripciÃ³n del Proyecto

Este proyecto utiliza **Docker Compose** para orquestar mÃºltiples servicios, incluyendo una API backend, una aplicaciÃ³n frontend y un entorno de desarrollo con JupyterLab. Se emplea un volumen compartido para almacenar modelos.

Adicionalmente, Este proyecto cuenta con tres servicios principales: un backend basado en FastAPI (api_back) que expone el **puerto 8000**, un frontend con Streamlit (api_front) en el **puerto 8501**, y un entorno de desarrollo interactivo con JupyterLab (jupyter) en el **puerto 8888**. Todos los servicios comparten el volumen models_volume para el almacenamiento de modelos.

---

## ğŸ›  TecnologÃ­as Utilizadas

- **ğŸ Python 3.10-slim**
- **ğŸš€ FastAPI** (Para la creaciÃ³n del API REST)
- **ğŸ¨ Streamlit** (Para la interfaz grÃ¡fica del usuario)
- **ğŸ“Š Scikit-learn** (Para entrenamiento de modelos de Machine Learning)
- **ğŸ³ Docker** (Para la contenedorizaciÃ³n y despliegue)
- **![image](https://github.com/user-attachments/assets/00de2200-517d-4029-b770-c5b29ffb6ab6) Docker Compose** (Para orquestar contenedores)
- **âš¡ Uvicorn** (Para ejecutar FastAPI)
- **ğŸ’¾ Joblib & Pickle** (Para serializaciÃ³n de modelos)

---

## ğŸš€ Instrucciones de InstalaciÃ³n y EjecuciÃ³n

### 1ï¸âƒ£ Clonar el Repositorio
```bash
 git clone <git@github.com:JohnSanchez27/MLOps_Taller2.git>
 cd <ml_deploy_fastapi/app/backend>
```

### 2ï¸âƒ£ Construir y Ejecutar el Contenedor Docker

Para construir y ejecutar los contenedores:
```bash
docker-compose up --build
```

Para detener los contenedores sin eliminarlos::
```bash
docker-compose stop
```
**Nota: VolÃºmenes Compartidos**: Este proyecto utiliza un volumen persistente models_volume para compartir modelos entre los diferentes servicios.

Esto iniciarÃ¡ la API en el puerto **8989** y la interfaz de Streamlit en el puerto **8501**.

### 3ï¸âƒ£ Acceder a la API, la Interfaz GrÃ¡fica y Jupyter

- ğŸ“Œ **API Docs**: Para acceder a la API Backend, visita: [http://localhost:8000]
- ğŸ–¥ **Interfaz Streamlit**: ara acceder a la interfaz en Streamlit, visita: [http://localhost:8501]
- ğŸ– **Jupyter** :Para acceder a JupyterLab, visita: [http://localhost:8888]

---

## ğŸ“‚ Estructura del Proyecto

La estructura del proyecto estÃ¡ organizada en diferentes directorios para separar los servicios y recursos. app_back contiene el cÃ³digo de la API backend con FastAPI, 
app_front alberga la interfaz de usuario construida con Streamlit, y jupyter proporciona un entorno interactivo con JupyterLab. AdemÃ¡s, models_volume actÃºa como almacenamiento 
compartido para modelos de machine learning en formato **.pkl.** El archivo **docker-compose.yml** define y configura la ejecuciÃ³n de todos los servicios en contenedores Docker.

```bash
Taller_2/
â”‚â”€â”€ app_back/
â”‚   â”‚â”€â”€ domain/
â”‚   â”‚â”€â”€ cargar_modelos.py
â”‚   â”‚â”€â”€ Dockerfile
â”‚   â”‚â”€â”€ main.py
â”‚   â”‚â”€â”€ requirements.txt
â”‚
â”‚â”€â”€ app_front/
â”‚   â”‚â”€â”€ app_streamlit.py
â”‚   â”‚â”€â”€ Dockerfile
â”‚   â”‚â”€â”€ requirements.txt
â”‚
â”‚â”€â”€ jupyter/
â”‚   â”‚â”€â”€ data/
â”‚   â”‚â”€â”€ Dockerfile
â”‚   â”‚â”€â”€ requirements.txt
â”‚
â”‚â”€â”€ models_volume/
â”‚   â”‚â”€â”€ notebook/
â”‚   â”‚â”€â”€ column_order.pkl
â”‚   â”‚â”€â”€ modelo1.pkl
â”‚   â”‚â”€â”€ modelo2.pkl
â”‚   â”‚â”€â”€ modelo3.pkl
â”‚   â”‚â”€â”€ modelo4.pkl
â”‚   â”‚â”€â”€ modelo22.pkl
â”‚   â”‚â”€â”€ modelo23.pkl
â”‚   â”‚â”€â”€ modelo24.pkl
â”‚
â”‚â”€â”€ docker-compose.yml
```

---

## ğŸ”— Funcionamiento del API

La API permite realizar inferencias sobre la especie de un pingÃ¼ino basÃ¡ndose en distintas caracterÃ­sticas. 

### **1ï¸âƒ£ Inferencia con todos los modelos**
ğŸ”¹ Devuelve la predicciÃ³n basada en el modelo con mayor consenso.
   - **ğŸ“Œ Endpoint:** `POST /pinguino`
   - **ğŸ“© Entrada:** Datos del pingÃ¼ino en formato JSON.
   - **ğŸ“¤ Salida:** PredicciÃ³n de la especie del pingÃ¼ino.

### **2ï¸âƒ£ Inferencia con un modelo especÃ­fico**
ğŸ”¹ El usuario puede seleccionar el modelo de Machine Learning a utilizar.
   - **ğŸ“Œ Endpoint:** `POST /pinguino/{modelo}`
   - **ğŸ“© Entrada:** Datos del pingÃ¼ino y el nombre del modelo a utilizar.
   - **ğŸ“¤ Salida:** PredicciÃ³n basada en el modelo seleccionado.

#### **Ejemplo de solicitud JSON:**
```json
{
  "culmen_length_mm": 50.0,
  "culmen_depth_mm": 18.0,
  "flipper_length_mm": 200.0,
  "body_mass_g": 5000,
  "island": "Torgersen",
  "sex": "MALE"
}
```

---

## ğŸ¤– Modelos Implementados

Se han entrenado cuatro modelos de Machine Learning:

- âœ… **K-Nearest Neighbors (modelo1)**
- âœ… **Support Vector Machine (modelo2)**
- âœ… **Naive Bayes (modelo3)**
- âœ… **PerceptrÃ³n Multicapa (modelo4)**

Estos modelos han sido entrenados utilizando el dataset de pingÃ¼inos y evaluados con una particiÃ³n de datos de entrenamiento y prueba, Sin embargo como se estan utilizando volumenes
se entrenaron 4 modelos adicionales con el fin de garantizar que se puedan agregar mas modelos y que la APP los pueda consumir. 

- âœ… **Ãrbol de DecisiÃ³n (modelo22)**
- âœ… **Random Forest o Bosque Aleatorio (modelo23)**
- âœ… **Gradient Boosting (modelo24)**
  
---

## ğŸ¨ ImplementaciÃ³n de Streamlit

La aplicaciÃ³n de **Streamlit** permite a los usuarios interactuar con la API de manera visual. Desde la interfaz, el usuario puede:

1ï¸âƒ£ Ingresar las caracterÃ­sticas de un pingÃ¼ino.  
2ï¸âƒ£ Seleccionar el modelo de Machine Learning.  
3ï¸âƒ£ Obtener la predicciÃ³n en tiempo real.  

AsÃ­ deberÃ¡s ver la interfaz:

![alt text](image.png)

---
